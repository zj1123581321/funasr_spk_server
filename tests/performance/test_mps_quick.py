"""
Mac GPU (MPS) åŠ é€Ÿå¿«é€Ÿæµ‹è¯•è„šæœ¬

å®Œæ•´æµç¨‹æµ‹è¯•ï¼šVAD + ASR + Speaker Diarization + Punctuation
æµ‹è¯• MPS å¯¹å®Œæ•´è½¬å½•æµç¨‹çš„åŠ é€Ÿæ•ˆæœ
"""
import os
import sys
import time
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from loguru import logger

# é…ç½®æ—¥å¿—
logger.remove()
logger.add(sys.stdout, level="INFO", format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}")


def patch_funasr_mps_support():
    """ä¸´æ—¶ä¿®å¤ FunASR çš„ MPS æ”¯æŒé—®é¢˜"""
    from funasr.auto import auto_model
    original_build_model = auto_model.AutoModel.build_model

    @staticmethod
    def patched_build_model(**kwargs):
        """ä¿®å¤åçš„ build_model æ–¹æ³•ï¼Œæ”¯æŒ MPS"""
        assert "model" in kwargs
        if "model_conf" not in kwargs:
            from funasr.download.download_model_from_hub import download_model
            import logging
            logging.info("download models from model hub: {}".format(kwargs.get("hub", "ms")))
            kwargs = download_model(**kwargs)

        from funasr.train_utils.set_all_random_seed import set_all_random_seed
        set_all_random_seed(kwargs.get("seed", 0))

        # ä¿®å¤ï¼šæ”¯æŒ MPS è®¾å¤‡
        device = kwargs.get("device", "cuda")

        if device == "cuda" and not torch.cuda.is_available():
            if torch.backends.mps.is_available() and torch.backends.mps.is_built():
                device = "mps"
                logger.info("CUDA ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ° MPS è®¾å¤‡")
            else:
                device = "cpu"
                logger.info("GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
        elif device == "mps":
            if not torch.backends.mps.is_available() or not torch.backends.mps.is_built():
                logger.warning("MPS ä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU")
                device = "cpu"
            else:
                logger.info("ä½¿ç”¨ MPS è®¾å¤‡è¿›è¡ŒåŠ é€Ÿ")

        # åªæœ‰åœ¨ CPU æ¨¡å¼ä¸‹æ‰å¼ºåˆ¶ batch_size=1
        if device == "cpu" and kwargs.get("ngpu", 1) == 0:
            kwargs["batch_size"] = 1

        kwargs["device"] = device
        torch.set_num_threads(kwargs.get("ncpu", 4))

        # ç»§ç»­åŸå§‹é€»è¾‘ï¼ˆæ„å»º tokenizer, frontend, modelï¼‰
        from funasr.register import tables
        from funasr.utils.misc import deep_update
        from funasr.train_utils.load_pretrained_model import load_pretrained_model
        from omegaconf import ListConfig

        # build tokenizer
        tokenizer = kwargs.get("tokenizer", None)
        kwargs["tokenizer"] = tokenizer
        kwargs["vocab_size"] = -1

        if tokenizer is not None:
            tokenizers = tokenizer.split(",") if isinstance(tokenizer, str) else tokenizer
            tokenizers_conf = kwargs.get("tokenizer_conf", {})
            tokenizers_build = []
            vocab_sizes = []
            token_lists = []

            token_list_files = kwargs.get("token_lists", [])
            seg_dicts = kwargs.get("seg_dicts", [])

            if not isinstance(tokenizers_conf, (list, tuple, ListConfig)):
                tokenizers_conf = [tokenizers_conf] * len(tokenizers)

            for i, tokenizer in enumerate(tokenizers):
                tokenizer_class = tables.tokenizer_classes.get(tokenizer)
                tokenizer_conf = tokenizers_conf[i]

                if len(token_list_files) > 1:
                    tokenizer_conf["token_list"] = token_list_files[i]
                if len(seg_dicts) > 1:
                    tokenizer_conf["seg_dict"] = seg_dicts[i]

                tokenizer = tokenizer_class(**tokenizer_conf)
                tokenizers_build.append(tokenizer)
                token_list = tokenizer.token_list if hasattr(tokenizer, "token_list") else None
                token_list = tokenizer.get_vocab() if hasattr(tokenizer, "get_vocab") else token_list
                vocab_size = -1
                if token_list is not None:
                    vocab_size = len(token_list)

                if vocab_size == -1 and hasattr(tokenizer, "get_vocab_size"):
                    vocab_size = tokenizer.get_vocab_size()
                token_lists.append(token_list)
                vocab_sizes.append(vocab_size)

            if len(tokenizers_build) <= 1:
                tokenizers_build = tokenizers_build[0]
                token_lists = token_lists[0]
                vocab_sizes = vocab_sizes[0]

            kwargs["tokenizer"] = tokenizers_build
            kwargs["vocab_size"] = vocab_sizes
            kwargs["token_list"] = token_lists

        # build frontend
        frontend = kwargs.get("frontend", None)
        kwargs["input_size"] = None
        if frontend is not None:
            frontend_class = tables.frontend_classes.get(frontend)
            frontend = frontend_class(**kwargs.get("frontend_conf", {}))
            kwargs["input_size"] = frontend.output_size() if hasattr(frontend, "output_size") else None
        kwargs["frontend"] = frontend

        # build model
        model_class = tables.model_classes.get(kwargs["model"])
        assert model_class is not None, f'{kwargs["model"]} is not registered'
        model_conf = {}
        deep_update(model_conf, kwargs.get("model_conf", {}))
        deep_update(model_conf, kwargs)
        model = model_class(**model_conf)

        # init_param
        init_param = kwargs.get("init_param", None)
        if init_param is not None:
            if os.path.exists(init_param):
                import logging
                logging.info(f"Loading pretrained params from {init_param}")
                load_pretrained_model(
                    model=model,
                    path=init_param,
                    ignore_init_mismatch=kwargs.get("ignore_init_mismatch", True),
                    oss_bucket=kwargs.get("oss_bucket", None),
                    scope_map=kwargs.get("scope_map", []),
                    excludes=kwargs.get("excludes", None),
                )

        # fp16/bf16
        if kwargs.get("fp16", False):
            model.to(torch.float16)
        elif kwargs.get("bf16", False):
            model.to(torch.bfloat16)
        model.to(device)

        if not kwargs.get("disable_log", True):
            tables.print()

        return model, kwargs

    # åº”ç”¨è¡¥ä¸
    auto_model.AutoModel.build_model = patched_build_model
    logger.success("âœ… FunASR MPS æ”¯æŒè¡¥ä¸å·²åº”ç”¨")


def get_audio_duration(audio_path: str) -> float:
    """è·å–éŸ³é¢‘æ—¶é•¿"""
    import subprocess
    try:
        result = subprocess.run(
            ['ffprobe', '-v', 'error', '-show_entries', 'format=duration',
             '-of', 'default=noprint_wrappers=1:nokey=1', audio_path],
            capture_output=True,
            text=True
        )
        return float(result.stdout.strip())
    except:
        return 0


def quick_test(audio_path: str, device: str):
    """å¿«é€Ÿæµ‹è¯•æŒ‡å®šè®¾å¤‡çš„æ€§èƒ½ï¼ˆå•æ¬¡æ¨ç†ï¼‰"""
    from funasr import AutoModel
    import json

    logger.info(f"\n{'='*60}")
    logger.info(f"æµ‹è¯•è®¾å¤‡: {device.upper()}")
    logger.info(f"{'='*60}\n")

    try:
        if not os.path.exists(audio_path):
            logger.error(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            return None

        audio_duration = get_audio_duration(audio_path)
        logger.info(f"éŸ³é¢‘æ–‡ä»¶: {os.path.basename(audio_path)}")
        logger.info(f"éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f} ç§’")

        # åˆå§‹åŒ–å®Œæ•´æ¨¡å‹ï¼ˆVAD + ASR + Speaker Diarization + Punctuationï¼‰
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–å®Œæ•´æ¨¡å‹ï¼ˆåŒ…å«è¯´è¯äººè¯†åˆ«ï¼‰...")
        start_init = time.time()

        model = AutoModel(
            model="paraformer-zh",
            model_revision="v2.0.4",
            vad_model="fsmn-vad",
            vad_model_revision="v2.0.4",
            punc_model="ct-punc-c",
            punc_model_revision="v2.0.4",
            spk_model="cam++",  # è¯´è¯äººè¯†åˆ«æ¨¡å‹
            spk_model_revision="v2.0.2",
            device=device,
            disable_update=True,
            disable_pbar=False  # æ˜¾ç¤ºè¿›åº¦æ¡
        )

        init_time = time.time() - start_init
        logger.info(f"æ¨¡å‹åˆå§‹åŒ–è€—æ—¶: {init_time:.2f} ç§’")

        # æ£€æŸ¥å®é™…ä½¿ç”¨çš„è®¾å¤‡
        actual_device = next(model.model.parameters()).device
        logger.success(f"âœ… å®é™…ä½¿ç”¨çš„è®¾å¤‡: {actual_device}")

        # å•æ¬¡æ¨ç†æµ‹è¯•
        logger.info("å¼€å§‹æ¨ç†æµ‹è¯•...")
        start_time = time.time()
        result = model.generate(input=audio_path, batch_size_s=300, hotword='')
        inference_time = time.time() - start_time

        rtf = inference_time / audio_duration if audio_duration > 0 else 0

        logger.success(f"\n{'='*60}")
        logger.success(f"æµ‹è¯•ç»“æœ ({device.upper()})")
        logger.success(f"{'='*60}")
        logger.success(f"æ¨ç†æ—¶é—´: {inference_time:.2f} ç§’")
        logger.success(f"RTF: {rtf:.4f}")
        logger.success(f"é€Ÿåº¦å€ç‡: {1/rtf:.2f}x")

        # ä¿å­˜è½¬å½•ç»“æœåˆ°æ–‡ä»¶
        if result and len(result) > 0:
            result_data = result[0]
            text = result_data.get('text', '')

            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = project_root / "tests" / "performance" / "output"
            output_dir.mkdir(parents=True, exist_ok=True)

            # 1. ä¿å­˜å®Œæ•´ JSON ç»“æœ
            json_file = output_dir / f"transcription_{device}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… å®Œæ•´ç»“æœå·²ä¿å­˜: {json_file}")

            # 2. ä¿å­˜çº¯æ–‡æœ¬
            txt_file = output_dir / f"transcription_{device}.txt"
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(text)
            logger.info(f"âœ… çº¯æ–‡æœ¬å·²ä¿å­˜: {txt_file}")

            # 3. ä¿å­˜å¸¦æ—¶é—´æˆ³å’Œè¯´è¯äººä¿¡æ¯çš„å¥å­
            if 'sentence_info' in result_data:
                sentences_file = output_dir / f"transcription_{device}_sentences.txt"
                with open(sentences_file, 'w', encoding='utf-8') as f:
                    for i, sent in enumerate(result_data['sentence_info'], 1):
                        start_ms = sent.get('start', 0)
                        end_ms = sent.get('end', 0)
                        text = sent.get('text', '')

                        # æå–è¯´è¯äºº - è½¬æ¢ä¸º Speaker1, Speaker2 æ ¼å¼
                        speaker_id = sent.get('spk', 0)
                        if isinstance(speaker_id, int):
                            speaker = f"Speaker{speaker_id + 1}"
                        else:
                            speaker = "Speaker1"

                        # è½¬æ¢æ—¶é—´æ ¼å¼
                        start_time = f"{start_ms//60000:02d}:{(start_ms%60000)//1000:02d}.{start_ms%1000:03d}"
                        end_time = f"{end_ms//60000:02d}:{(end_ms%60000)//1000:02d}.{end_ms%1000:03d}"

                        f.write(f"[{i}] {start_time} -> {end_time} | {speaker}\n")
                        f.write(f"{text}\n\n")
                logger.info(f"âœ… å¸¦æ—¶é—´æˆ³å’Œè¯´è¯äººä¿¡æ¯å·²ä¿å­˜: {sentences_file}")

            logger.info(f"\nè½¬å½•ç»“æœé¢„è§ˆ:\n{text[:200]}...\n")

        return {
            'device': device,
            'audio_duration': audio_duration,
            'init_time': init_time,
            'inference_time': inference_time,
            'rtf': rtf,
            'speedup': 1/rtf if rtf > 0 else 0,
            'actual_device': str(actual_device),
            'result': result[0] if result else None
        }

    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥ ({device}): {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    # åº”ç”¨ MPS æ”¯æŒè¡¥ä¸
    patch_funasr_mps_support()

    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    audio_file = os.path.join(project_root, "temp", "test.m4a")

    if not os.path.exists(audio_file):
        logger.error(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
        logger.info("è¯·å°†æµ‹è¯•éŸ³é¢‘æ–‡ä»¶æ”¾ç½®åœ¨ temp/test.m4a")
        return

    # æ£€æŸ¥è®¾å¤‡æ”¯æŒ
    logger.info("æ£€æŸ¥è®¾å¤‡æ”¯æŒæƒ…å†µ...")
    logger.info(f"  CUDA å¯ç”¨: {torch.cuda.is_available()}")
    logger.info(f"  MPS å¯ç”¨: {torch.backends.mps.is_available()}")
    logger.info(f"  MPS å·²æ„å»º: {torch.backends.mps.is_built()}")

    results = {}

    # æµ‹è¯• CPU
    logger.info("\n" + "ğŸ”µ æµ‹è¯• 1/2: CPU æ¨¡å¼")
    results['cpu'] = quick_test(audio_file, "cpu")

    # æµ‹è¯• MPS
    if torch.backends.mps.is_available():
        logger.info("\n" + "ğŸŸ¢ æµ‹è¯• 2/2: MPS æ¨¡å¼")
        results['mps'] = quick_test(audio_file, "mps")
    else:
        logger.warning("âš ï¸  MPS ä¸å¯ç”¨ï¼Œè·³è¿‡ MPS æµ‹è¯•")

    # æ€§èƒ½å¯¹æ¯”
    logger.info(f"\n{'='*80}")
    logger.info("æ€§èƒ½å¯¹æ¯”åˆ†æ")
    logger.info(f"{'='*80}\n")

    if results['cpu'] and results.get('mps'):
        cpu_time = results['cpu']['inference_time']
        mps_time = results['mps']['inference_time']
        speedup = cpu_time / mps_time

        logger.info(f"{'è®¾å¤‡':<15} {'æ¨ç†æ—¶é—´':<15} {'RTF':<15} {'ç›¸å¯¹åŠ é€Ÿ':<15}")
        logger.info("-" * 80)
        logger.info(f"{'CPU':<15} {cpu_time:>10.2f}s    {results['cpu']['rtf']:>10.4f}    {'1.00x (baseline)':<15}")

        if speedup > 1.0:
            logger.success(f"{'MPS':<15} {mps_time:>10.2f}s    {results['mps']['rtf']:>10.4f}    {speedup:>10.2f}x")
        else:
            logger.warning(f"{'MPS':<15} {mps_time:>10.2f}s    {results['mps']['rtf']:>10.4f}    {speedup:>10.2f}x")

        logger.info("\n" + "="*80)
        logger.info("ğŸ’¡ ç»“è®ºä¸å»ºè®®")
        logger.info("="*80)

        if speedup > 1.5:
            logger.success(f"âœ… MPS åŠ é€Ÿæ˜¾è‘—ï¼ç›¸æ¯” CPU æå‡ {speedup:.2f}x")
            logger.info("\nå»ºè®®æ“ä½œï¼š")
            logger.info("1. ä¿®æ”¹ config.jsonï¼Œè®¾ç½® \"device\": \"mps\"")
            logger.info("2. åº”ç”¨ MPS è¡¥ä¸åˆ° FunASR æºç ï¼ˆè§ä¸‹æ–¹è¯´æ˜ï¼‰")
        elif speedup > 1.1:
            logger.info(f"âš ï¸  MPS æœ‰å°å¹…æå‡ ({speedup:.2f}x)")
            logger.info("å»ºè®®ï¼šæ ¹æ®å®é™…éœ€æ±‚å†³å®šæ˜¯å¦å¯ç”¨ MPS")
        else:
            logger.warning(f"âŒ MPS æ€§èƒ½ä¸ä½³ï¼ˆä»… {speedup:.2f}xï¼‰")
            logger.info("å»ºè®®ï¼šç»§ç»­ä½¿ç”¨ CPU æ¨¡å¼")

        logger.info("\n" + "="*80)
        logger.info("ğŸ“ å¦‚ä½•æ°¸ä¹…å¯ç”¨ MPS åŠ é€Ÿ")
        logger.info("="*80)
        logger.info("\næ–¹æ³•ï¼šä¿®æ”¹ FunASR æºç ")
        logger.info(f"æ–‡ä»¶ä½ç½®: {project_root}/venv/lib/python3.11/site-packages/funasr/auto/auto_model.py")
        logger.info("\næ‰¾åˆ°ç¬¬ 185-187 è¡Œï¼Œæ³¨é‡Šæ‰å¼ºåˆ¶ CPU å›é€€é€»è¾‘ï¼š")
        logger.info("""
# ä¿®æ”¹å‰ï¼š
if not torch.cuda.is_available() or kwargs.get("ngpu", 1) == 0:
    device = "cpu"
    kwargs["batch_size"] = 1

# ä¿®æ”¹åï¼š
# æ”¯æŒ MPS è®¾å¤‡
if device == "cuda" and not torch.cuda.is_available():
    if torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
        kwargs["batch_size"] = 1
elif device == "cpu" and kwargs.get("ngpu", 1) == 0:
    kwargs["batch_size"] = 1
        """)

    elif results['cpu']:
        logger.info("ä»…å®Œæˆ CPU æµ‹è¯•")

    # å¯¹æ¯”è½¬å½•ç»“æœ
    if results.get('cpu') and results.get('mps'):
        logger.info("\n" + "="*80)
        logger.info("ğŸ“„ è½¬å½•ç»“æœå¯¹æ¯”")
        logger.info("="*80)

        output_dir = project_root / "tests" / "performance" / "output"

        # å¯¹æ¯”æ–‡æœ¬å·®å¼‚
        cpu_txt = output_dir / "transcription_cpu.txt"
        mps_txt = output_dir / "transcription_mps.txt"

        if cpu_txt.exists() and mps_txt.exists():
            with open(cpu_txt, 'r', encoding='utf-8') as f:
                cpu_text = f.read()
            with open(mps_txt, 'r', encoding='utf-8') as f:
                mps_text = f.read()

            if cpu_text == mps_text:
                logger.success("âœ… CPU å’Œ MPS è½¬å½•ç»“æœå®Œå…¨ä¸€è‡´")
            else:
                logger.warning("âš ï¸  CPU å’Œ MPS è½¬å½•ç»“æœå­˜åœ¨å·®å¼‚")
                logger.info(f"CPU æ–‡æœ¬é•¿åº¦: {len(cpu_text)} å­—ç¬¦")
                logger.info(f"MPS æ–‡æœ¬é•¿åº¦: {len(mps_text)} å­—ç¬¦")

                # ç®€å•çš„å·®å¼‚ç»Ÿè®¡
                import difflib
                diff_ratio = difflib.SequenceMatcher(None, cpu_text, mps_text).ratio()
                logger.info(f"ç›¸ä¼¼åº¦: {diff_ratio*100:.2f}%")

        logger.info(f"\nè¾“å‡ºæ–‡ä»¶ä½ç½®: {output_dir}")
        logger.info("ç”Ÿæˆçš„æ–‡ä»¶:")
        logger.info("  - transcription_cpu.json (CPU å®Œæ•´ç»“æœ)")
        logger.info("  - transcription_cpu.txt (CPU çº¯æ–‡æœ¬)")
        logger.info("  - transcription_cpu_sentences.txt (CPU å¸¦æ—¶é—´æˆ³)")
        logger.info("  - transcription_mps.json (MPS å®Œæ•´ç»“æœ)")
        logger.info("  - transcription_mps.txt (MPS çº¯æ–‡æœ¬)")
        logger.info("  - transcription_mps_sentences.txt (MPS å¸¦æ—¶é—´æˆ³)")


if __name__ == "__main__":
    main()
