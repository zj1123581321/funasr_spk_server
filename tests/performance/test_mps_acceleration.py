"""
Mac GPU (MPS) åŠ é€Ÿæ€§èƒ½æµ‹è¯•è„šæœ¬

åŸºäº GitHub Issue #1802 çš„è§£å†³æ–¹æ¡ˆï¼Œæµ‹è¯• FunASR åœ¨ Apple Silicon ä¸Šçš„ GPU åŠ é€Ÿæ•ˆæœ
"""
import os
import sys
import time
import torch
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from loguru import logger

# é…ç½®æ—¥å¿—
logger.remove()
logger.add(sys.stdout, level="INFO", format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}")


def patch_funasr_mps_support():
    """
    ä¸´æ—¶ä¿®å¤ FunASR çš„ MPS æ”¯æŒé—®é¢˜

    æ ¹æ® GitHub Issue #1802ï¼ŒFunASR ä¼šå¼ºåˆ¶å›é€€åˆ° CPUï¼Œéœ€è¦ä¿®æ”¹ build_model æ–¹æ³•
    """
    from funasr.auto import auto_model

    original_build_model = auto_model.AutoModel.build_model

    @staticmethod
    def patched_build_model(**kwargs):
        """ä¿®å¤åçš„ build_model æ–¹æ³•ï¼Œæ”¯æŒ MPS"""
        assert "model" in kwargs
        if "model_conf" not in kwargs:
            from funasr.download.download_model_from_hub import download_model
            import logging
            logging.info("download models from model hub: {}".format(kwargs.get("hub", "ms")))
            kwargs = download_model(**kwargs)

        from funasr.train_utils.set_all_random_seed import set_all_random_seed
        set_all_random_seed(kwargs.get("seed", 0))

        # ä¿®å¤ï¼šæ”¯æŒ MPS è®¾å¤‡
        device = kwargs.get("device", "cuda")

        # æ£€æŸ¥ CUDA
        if device == "cuda" and not torch.cuda.is_available():
            # å°è¯•ä½¿ç”¨ MPS
            if torch.backends.mps.is_available() and torch.backends.mps.is_built():
                device = "mps"
                logger.info("CUDA ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ° MPS è®¾å¤‡")
            else:
                device = "cpu"
                logger.info("GPU ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")

        # å¦‚æœæ˜¾å¼æŒ‡å®šäº† MPSï¼Œä¸è¦å›é€€åˆ° CPU
        elif device == "mps":
            if not torch.backends.mps.is_available() or not torch.backends.mps.is_built():
                logger.warning("MPS ä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU")
                device = "cpu"
            else:
                logger.info("ä½¿ç”¨ MPS è®¾å¤‡è¿›è¡ŒåŠ é€Ÿ")

        # åªæœ‰åœ¨ CPU æ¨¡å¼ä¸‹æ‰å¼ºåˆ¶ batch_size=1
        if device == "cpu" and kwargs.get("ngpu", 1) == 0:
            kwargs["batch_size"] = 1

        kwargs["device"] = device
        torch.set_num_threads(kwargs.get("ncpu", 4))

        # ç»§ç»­åŸå§‹é€»è¾‘ï¼ˆæ„å»º tokenizer, frontend, modelï¼‰
        from funasr.register import tables
        from funasr.utils.misc import deep_update
        from funasr.train_utils.load_pretrained_model import load_pretrained_model
        from omegaconf import ListConfig

        # build tokenizer
        tokenizer = kwargs.get("tokenizer", None)
        kwargs["tokenizer"] = tokenizer
        kwargs["vocab_size"] = -1

        if tokenizer is not None:
            tokenizers = (
                tokenizer.split(",") if isinstance(tokenizer, str) else tokenizer
            )
            tokenizers_conf = kwargs.get("tokenizer_conf", {})
            tokenizers_build = []
            vocab_sizes = []
            token_lists = []

            token_list_files = kwargs.get("token_lists", [])
            seg_dicts = kwargs.get("seg_dicts", [])

            if not isinstance(tokenizers_conf, (list, tuple, ListConfig)):
                tokenizers_conf = [tokenizers_conf] * len(tokenizers)

            for i, tokenizer in enumerate(tokenizers):
                tokenizer_class = tables.tokenizer_classes.get(tokenizer)
                tokenizer_conf = tokenizers_conf[i]

                if len(token_list_files) > 1:
                    tokenizer_conf["token_list"] = token_list_files[i]
                if len(seg_dicts) > 1:
                    tokenizer_conf["seg_dict"] = seg_dicts[i]

                tokenizer = tokenizer_class(**tokenizer_conf)
                tokenizers_build.append(tokenizer)
                token_list = tokenizer.token_list if hasattr(tokenizer, "token_list") else None
                token_list = (
                    tokenizer.get_vocab() if hasattr(tokenizer, "get_vocab") else token_list
                )
                vocab_size = -1
                if token_list is not None:
                    vocab_size = len(token_list)

                if vocab_size == -1 and hasattr(tokenizer, "get_vocab_size"):
                    vocab_size = tokenizer.get_vocab_size()
                token_lists.append(token_list)
                vocab_sizes.append(vocab_size)

            if len(tokenizers_build) <= 1:
                tokenizers_build = tokenizers_build[0]
                token_lists = token_lists[0]
                vocab_sizes = vocab_sizes[0]

            kwargs["tokenizer"] = tokenizers_build
            kwargs["vocab_size"] = vocab_sizes
            kwargs["token_list"] = token_lists

        # build frontend
        frontend = kwargs.get("frontend", None)
        kwargs["input_size"] = None
        if frontend is not None:
            frontend_class = tables.frontend_classes.get(frontend)
            frontend = frontend_class(**kwargs.get("frontend_conf", {}))
            kwargs["input_size"] = (
                frontend.output_size() if hasattr(frontend, "output_size") else None
            )
        kwargs["frontend"] = frontend

        # build model
        model_class = tables.model_classes.get(kwargs["model"])
        assert model_class is not None, f'{kwargs["model"]} is not registered'
        model_conf = {}
        deep_update(model_conf, kwargs.get("model_conf", {}))
        deep_update(model_conf, kwargs)
        model = model_class(**model_conf)

        # init_param
        init_param = kwargs.get("init_param", None)
        if init_param is not None:
            if os.path.exists(init_param):
                import logging
                logging.info(f"Loading pretrained params from {init_param}")
                load_pretrained_model(
                    model=model,
                    path=init_param,
                    ignore_init_mismatch=kwargs.get("ignore_init_mismatch", True),
                    oss_bucket=kwargs.get("oss_bucket", None),
                    scope_map=kwargs.get("scope_map", []),
                    excludes=kwargs.get("excludes", None),
                )
            else:
                print(f"error, init_param does not exist!: {init_param}")

        # fp16
        if kwargs.get("fp16", False):
            model.to(torch.float16)
        elif kwargs.get("bf16", False):
            model.to(torch.bfloat16)
        model.to(device)

        if not kwargs.get("disable_log", True):
            tables.print()

        return model, kwargs

    # åº”ç”¨è¡¥ä¸
    auto_model.AutoModel.build_model = patched_build_model
    logger.success("âœ… FunASR MPS æ”¯æŒè¡¥ä¸å·²åº”ç”¨")


def get_audio_duration(audio_path: str) -> float:
    """è·å–éŸ³é¢‘æ—¶é•¿"""
    import subprocess
    try:
        result = subprocess.run(
            ['ffprobe', '-v', 'error', '-show_entries', 'format=duration',
             '-of', 'default=noprint_wrappers=1:nokey=1', audio_path],
            capture_output=True,
            text=True
        )
        return float(result.stdout.strip())
    except Exception as e:
        logger.error(f"è·å–éŸ³é¢‘æ—¶é•¿å¤±è´¥: {e}")
        return 0


def test_device_performance(audio_path: str, device: str, use_speaker: bool = True):
    """
    æµ‹è¯•æŒ‡å®šè®¾å¤‡çš„æ€§èƒ½

    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡ç±»å‹ (cpu, mps, cuda)
        use_speaker: æ˜¯å¦å¯ç”¨è¯´è¯äººè¯†åˆ«
    """
    from funasr import AutoModel

    logger.info(f"\n{'='*60}")
    logger.info(f"æµ‹è¯•è®¾å¤‡: {device.upper()}")
    logger.info(f"è¯´è¯äººè¯†åˆ«: {'å¯ç”¨' if use_speaker else 'ç¦ç”¨'}")
    logger.info(f"{'='*60}\n")

    try:
        # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶
        if not os.path.exists(audio_path):
            logger.error(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            return None

        audio_duration = get_audio_duration(audio_path)
        logger.info(f"éŸ³é¢‘æ–‡ä»¶: {os.path.basename(audio_path)}")
        logger.info(f"éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f} ç§’")

        # åˆå§‹åŒ–æ¨¡å‹
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
        start_init = time.time()

        if use_speaker:
            # åŒ…å«è¯´è¯äººè¯†åˆ«çš„å®Œæ•´æ¨¡å‹
            model = AutoModel(
                model="paraformer-zh",
                model_revision="v2.0.4",
                vad_model="fsmn-vad",
                vad_model_revision="v2.0.4",
                punc_model="ct-punc-c",
                punc_model_revision="v2.0.4",
                spk_model="cam++",
                spk_model_revision="v2.0.2",
                device=device,
                disable_update=True,
                disable_pbar=True
            )
        else:
            # ä»… ASR æ¨¡å‹ï¼ˆä¸å«è¯´è¯äººè¯†åˆ«ï¼‰
            model = AutoModel(
                model="paraformer-zh",
                model_revision="v2.0.4",
                vad_model="fsmn-vad",
                vad_model_revision="v2.0.4",
                punc_model="ct-punc-c",
                punc_model_revision="v2.0.4",
                device=device,
                disable_update=True,
                disable_pbar=True
            )

        init_time = time.time() - start_init
        logger.info(f"æ¨¡å‹åˆå§‹åŒ–è€—æ—¶: {init_time:.2f} ç§’")

        # æ£€æŸ¥å®é™…ä½¿ç”¨çš„è®¾å¤‡
        actual_device = next(model.model.parameters()).device
        logger.info(f"å®é™…ä½¿ç”¨çš„è®¾å¤‡: {actual_device}")

        # é¢„çƒ­ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢ï¼‰
        logger.info("é¢„çƒ­ä¸­...")
        _ = model.generate(input=audio_path, batch_size_s=300, hotword='')

        # æ­£å¼æµ‹è¯•ï¼ˆè¿è¡Œ3æ¬¡å–å¹³å‡å€¼ï¼‰
        logger.info("å¼€å§‹æ€§èƒ½æµ‹è¯•...")
        inference_times = []

        for i in range(3):
            start_time = time.time()
            result = model.generate(input=audio_path, batch_size_s=300, hotword='')
            inference_time = time.time() - start_time
            inference_times.append(inference_time)

            rtf = inference_time / audio_duration if audio_duration > 0 else 0
            logger.info(f"  ç¬¬ {i+1} æ¬¡: {inference_time:.2f}s (RTF: {rtf:.4f})")

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        avg_time = sum(inference_times) / len(inference_times)
        avg_rtf = avg_time / audio_duration if audio_duration > 0 else 0

        logger.success(f"\n{'='*60}")
        logger.success(f"æµ‹è¯•ç»“æœæ±‡æ€» ({device.upper()})")
        logger.success(f"{'='*60}")
        logger.success(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} ç§’")
        logger.success(f"å¹³å‡ RTF: {avg_rtf:.4f}")
        logger.success(f"é€Ÿåº¦å€ç‡: {1/avg_rtf:.2f}x (ç›¸å¯¹å®æ—¶)")

        # æå–è½¬å½•ç»“æœ
        if result and len(result) > 0:
            text = result[0].get('text', '')
            logger.info(f"\nè½¬å½•ç»“æœé¢„è§ˆ:\n{text[:200]}...")

            if 'sentence_info' in result[0]:
                sentence_count = len(result[0]['sentence_info'])
                logger.info(f"å¥å­æ•°é‡: {sentence_count}")

        return {
            'device': device,
            'use_speaker': use_speaker,
            'audio_duration': audio_duration,
            'init_time': init_time,
            'inference_times': inference_times,
            'avg_inference_time': avg_time,
            'avg_rtf': avg_rtf,
            'speedup': 1/avg_rtf if avg_rtf > 0 else 0,
            'actual_device': str(actual_device)
        }

    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥ ({device}): {e}")
        import traceback
        traceback.print_exc()
        return None


def compare_performance(results: dict):
    """å¯¹æ¯”ä¸åŒè®¾å¤‡çš„æ€§èƒ½"""
    if not results:
        return

    logger.info(f"\n{'='*80}")
    logger.info("æ€§èƒ½å¯¹æ¯”åˆ†æ")
    logger.info(f"{'='*80}\n")

    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    header = f"{'è®¾å¤‡':<15} {'è¯´è¯äºº':<10} {'å¹³å‡è€—æ—¶':<15} {'RTF':<15} {'åŠ é€Ÿæ¯”':<15}"
    logger.info(header)
    logger.info("-" * 80)

    baseline_time = None
    for key, result in results.items():
        if result:
            device_name = result['device'].upper()
            use_spk = "âœ“" if result['use_speaker'] else "âœ—"
            avg_time = result['avg_inference_time']
            rtf = result['avg_rtf']
            speedup = result['speedup']

            # è®¡ç®—ç›¸å¯¹äº CPU çš„åŠ é€Ÿæ¯”
            if baseline_time is None:
                baseline_time = avg_time
                relative_speedup = "1.00x (baseline)"
            else:
                relative_speedup = f"{baseline_time / avg_time:.2f}x"

            row = f"{device_name:<15} {use_spk:<10} {avg_time:>10.2f}s    {rtf:>10.4f}    {relative_speedup:<15}"

            if device_name == "MPS" and relative_speedup != "1.00x (baseline)":
                logger.success(row)  # MPS åŠ é€Ÿç»“æœç”¨ç»¿è‰²é«˜äº®
            else:
                logger.info(row)

    logger.info("\n" + "="*80)


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    # åº”ç”¨ MPS æ”¯æŒè¡¥ä¸
    patch_funasr_mps_support()

    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    audio_file = os.path.join(project_root, "temp", "test.m4a")

    if not os.path.exists(audio_file):
        logger.error(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
        logger.info("è¯·å°†æµ‹è¯•éŸ³é¢‘æ–‡ä»¶æ”¾ç½®åœ¨ temp/test.m4a")
        return

    # æ£€æŸ¥è®¾å¤‡æ”¯æŒ
    logger.info("æ£€æŸ¥è®¾å¤‡æ”¯æŒæƒ…å†µ...")
    logger.info(f"  CUDA å¯ç”¨: {torch.cuda.is_available()}")
    logger.info(f"  MPS å¯ç”¨: {torch.backends.mps.is_available()}")
    logger.info(f"  MPS å·²æ„å»º: {torch.backends.mps.is_built()}")

    results = {}

    # æµ‹è¯• 1: CPU (ä¸å«è¯´è¯äººè¯†åˆ«) - åŸºå‡†æµ‹è¯•
    logger.info("\n" + "ğŸ”µ æµ‹è¯• 1/4: CPU æ¨¡å¼ï¼ˆä¸å«è¯´è¯äººè¯†åˆ«ï¼‰")
    results['cpu_no_spk'] = test_device_performance(audio_file, "cpu", use_speaker=False)

    # æµ‹è¯• 2: CPU (å«è¯´è¯äººè¯†åˆ«)
    logger.info("\n" + "ğŸ”µ æµ‹è¯• 2/4: CPU æ¨¡å¼ï¼ˆå«è¯´è¯äººè¯†åˆ«ï¼‰")
    results['cpu_with_spk'] = test_device_performance(audio_file, "cpu", use_speaker=True)

    # æµ‹è¯• 3: MPS (ä¸å«è¯´è¯äººè¯†åˆ«)
    if torch.backends.mps.is_available():
        logger.info("\n" + "ğŸŸ¢ æµ‹è¯• 3/4: MPS æ¨¡å¼ï¼ˆä¸å«è¯´è¯äººè¯†åˆ«ï¼‰")
        results['mps_no_spk'] = test_device_performance(audio_file, "mps", use_speaker=False)

        # æµ‹è¯• 4: MPS (å«è¯´è¯äººè¯†åˆ«) - æ³¨æ„: spk_model å¯èƒ½ä¸å…¼å®¹
        logger.info("\n" + "ğŸŸ¢ æµ‹è¯• 4/4: MPS æ¨¡å¼ï¼ˆå«è¯´è¯äººè¯†åˆ«ï¼‰")
        logger.warning("âš ï¸  æ ¹æ® Issue #1802ï¼Œè¯´è¯äººæ¨¡å‹å¯èƒ½ä¸å…¼å®¹ MPS")
        results['mps_with_spk'] = test_device_performance(audio_file, "mps", use_speaker=True)
    else:
        logger.warning("âš ï¸  MPS ä¸å¯ç”¨ï¼Œè·³è¿‡ MPS æµ‹è¯•")

    # æ€§èƒ½å¯¹æ¯”
    compare_performance(results)

    # å»ºè®®
    logger.info("\n" + "="*80)
    logger.info("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
    logger.info("="*80)

    if 'mps_with_spk' in results and results['mps_with_spk']:
        cpu_time = results['cpu_with_spk']['avg_inference_time']
        mps_time = results['mps_with_spk']['avg_inference_time']
        speedup = cpu_time / mps_time

        if speedup > 1.3:
            logger.success(f"âœ… MPS åŠ é€Ÿæœ‰æ•ˆï¼ç›¸æ¯” CPU æå‡ {speedup:.2f}x")
            logger.info("å»ºè®®ï¼šä¿®æ”¹ config.jsonï¼Œè®¾ç½® \"device\": \"mps\"")
        elif speedup > 1.0:
            logger.info(f"âš ï¸  MPS æœ‰å°å¹…æå‡ ({speedup:.2f}x)ï¼Œä½†æå‡ä¸æ˜æ˜¾")
            logger.info("å»ºè®®ï¼šæ ¹æ®å®é™…åœºæ™¯å†³å®šæ˜¯å¦å¯ç”¨ MPS")
        else:
            logger.warning("âŒ MPS åè€Œæ›´æ…¢ï¼Œå»ºè®®ç»§ç»­ä½¿ç”¨ CPU")

    logger.info("\nå¦‚éœ€åº”ç”¨ MPS åŠ é€Ÿï¼Œè¯·å‚è€ƒä»¥ä¸‹æ­¥éª¤:")
    logger.info("1. å¤‡ä»½ FunASR æºç æ–‡ä»¶")
    logger.info("2. ä¿®æ”¹ auto_model.py ä¸­çš„è®¾å¤‡æ£€æµ‹é€»è¾‘")
    logger.info("3. æ›´æ–° config.json: \"device\": \"mps\"")


if __name__ == "__main__":
    main()
